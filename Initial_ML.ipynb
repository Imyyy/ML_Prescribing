{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim to get as many initial ML models applied to the dataset using CDM and ML practical code.\n",
    "Can then go and refine these models later, just want to get them made.  \n",
    "ML coursework and practicals very useful, need to go and apply this to my dataset though.   \n",
    "Have added in the code from the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "os.chdir('../Data') \n",
    "toycomp = pd.read_csv('Combined_TOYCOMP_BNF_NHS_data.csv') # Using the BNF version of the dataframe\n",
    "def rename_unname(df):\n",
    "    for col in df:\n",
    "        if col.startswith('Unnamed'):\n",
    "            df.drop(col,axis=1, inplace=True)\n",
    "rename_unname(toycomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = plt.matshow(data.corr())\n",
    "plt.colorbar(orientation = 'vertical')\n",
    "plt.title('Correlation Matrix', fontsize=16)\n",
    "plt.xticks(range(len(data.columns)), data.columns, fontsize=5)\n",
    "plt.yticks(range(len(data.columns)), data.columns, fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :13]\n",
    "y = data.iloc[:, 13]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linearRegressor = LinearRegression()\n",
    "linearRegressor.fit(X_train, y_train) #Work out the best values in the model based on the training set\n",
    "y_pred = linearRegressor.predict(X_train)\n",
    "\n",
    "plt.scatter(y_train, y_pred)\n",
    "plt.title('Comparing the data point and the predicted value')\n",
    "plt.xlabel('Actual y value')\n",
    "plt.ylabel('Predicted y value')\n",
    "plt.show()\n",
    "\n",
    "#Evaluating this model using mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#RMSE training set\n",
    "print(np.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "\n",
    "#RMSE test\n",
    "y_predtest = linearRegressor.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, y_predtest)))\n",
    "\n",
    "#R2 training\n",
    "print(r2_score(y_train, y_pred))\n",
    "\n",
    "#R2 test\n",
    "print(r2_score(y_test, y_predtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative classisification using least squares, from ML practical 1\n",
    "\n",
    "# Create all possible combinations of attributes. \n",
    "\n",
    "# Itertools is a great python library that lets you deal with iterables in efficient ways. \n",
    "from itertools import chain, combinations\n",
    "def all_combinations(attributes):\n",
    "    return chain(*map(lambda i: combinations(attributes, i), range(1, len(attributes)+1)))\n",
    "\n",
    "_attributes = [name for name in column_names if name != 'class']\n",
    "attribute_combinations = all_combinations(_attributes) #Note that this is an iterable object. \n",
    "\n",
    "# Write a function that takes in a list of attributes, and outputs the predictions after carrying out least squares.\n",
    "def return_predictions(attributes, training_data=training_data, testing_data=test_data):    \n",
    "    \n",
    "    X = training_data[attributes].values.reshape(-1, len(attributes))\n",
    "    _ = np.tile(np.array([1]), [X.shape[0]]).reshape(-1,1)\n",
    "    X = np.append(_, X, axis=1)\n",
    "    \n",
    "    Y = training_data[\"output\"].values.reshape(-1, 1)\n",
    "    \n",
    "    X_test = test_data[attributes].values.reshape(-1, len(attributes))\n",
    "    _ = np.tile(np.array([1]), [X_test.shape[0]]).reshape(-1,1)\n",
    "    X_test = np.append(_, X_test, axis=1)\n",
    "    \n",
    "    # Least squares solution\n",
    "    W_opt = np.linalg.solve(np.matmul(X.T, X), np.matmul(X.T, Y))\n",
    "\n",
    "    predictions = np.matmul(X_test, W_opt)\n",
    "    \n",
    "    return predictions\n",
    "# Write a function that takes in a predictions vector, and outputs the mean squared error.\n",
    "def return_mse(predictions, testing_data=test_data):\n",
    "    Y_test = test_data[\"output\"].values.reshape(-1, 1)\n",
    "    \n",
    "    error = Y_test - predictions\n",
    "\n",
    "    square_error = np.square(error)\n",
    "    \n",
    "    mse = np.mean(square_error)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# evaluate\n",
    "attribute_combinations = all_combinations(_attributes)\n",
    "for attributes in attribute_combinations:\n",
    "    preds = return_predictions(list(attributes))\n",
    "    print(f\"{str(attributes):<70} MSE: {return_mse(preds)}\")\n",
    "    attribute_combinations = all_combinations(_attributes)\n",
    "\n",
    "for attributes in attribute_combinations:\n",
    "    preds = return_predictions(list(attributes))\n",
    "    print(f\"{str(attributes):<70} MSE: {return_mse(preds)}\")\n",
    "\n",
    "print(*attribute_combinations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbours\n",
    "clf = neighbors.KNeighborsClassifier() # Making that function where the object becomes callable\n",
    "clf.fit(X_train, y_train) # Fitting the model\n",
    "y_pred = clf.predict(X_test[:100]) # Creating the predictions for the test set\n",
    "accuracy = np.sum(y_pred == y_test[:100]) / len(y_pred) # Comparing to truth to work out accuracy of model\n",
    "idx_wrong = np.nonzero(y_pred != y_test[:100]) # Creating a group of all the ones that were classified wrong\n",
    "print('Accuracy = {0:.1f}%.'.format(accuracy * 100)) #Print out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the X_test data\n",
    "scaler = StandardScaler() #Only need to scale the train, then apply to the test data\n",
    "scaled_X_test = scaler.fit(X_test)\n",
    "scaled_X_test = scaler.transform(X_test) \n",
    "\n",
    "#Apply the PCA model created earlier to the x_test\n",
    "X_test2 = pca.transform(scaled_X_test) #Need to do the same idea to training data\n",
    "X_test2_2d = pd.DataFrame(X_test2) #Uses pandas to create a dataframe\n",
    "X_test2_2d.columns = ['PC1','PC2', 'PC3', 'PC4', 'PC5']\n",
    "X_test2_2d.head()\n",
    "\n",
    "#Therefore my new data set is called data_df_2d\n",
    "#This looks like a good website if I don't want to use PCA: https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n",
    "#Think I then need to train a model on this reduced data set\n",
    "#y_pred2 = linearRegressor.predict(data_df_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is large code file copy paste from the SVM practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# consider two classes of points which are well separated\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.50)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Task 1: Attempt to use linear regression to separate this data using linear regression.\n",
    "# Note there are several possibilities which separate the data?     \n",
    "# What happens to the classification of point [0.6, 2.1] (or similar)?\n",
    "\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "    \n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)    \n",
    "\n",
    "plt.xlim(-1, 3.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# With SVM rather than simply drawing a zero-width line between the \n",
    "# classes, we draw a margin of some width around each line, up to the nearest point. \n",
    "# For example for these lines:\n",
    "\n",
    "\"\"\" \n",
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Task 2: Draw the margin around the lines you chose in Task 1.\n",
    "\n",
    "#%%Cell\n",
    "\n",
    "\n",
    "# For SVM the line that maximises the margin is the optimal model\n",
    "\n",
    "# Task 3: Use the sklearn package to build a support vector classifier using a linear kernel\n",
    "# (hint: you will need from sklearn.svm import SVC). Plot the decision fuction on the data\n",
    "\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "\n",
    "def plot_svc_decision_function(model, ax=None):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "\n",
    "    ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "model = SVC(kernel='linear', C=1E10, gamma = 0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model)\n",
    "plt.show()\n",
    "\n",
    "#%% \n",
    "\n",
    "# Task 4: Change the number of points in the dataset using X = X[:N] and y = y[:N]\n",
    "# and build the classifier again using a linear kernel\n",
    "# Plot the decision function. Do you see any differences?\n",
    "\n",
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.50)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10, gamma = 0.1)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))\n",
    "plt.show()\n",
    "    \n",
    "## So far we have considered linear boundaries but this is not always the case\n",
    "\n",
    "## Consider the new dataset\n",
    "    \n",
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X2, y2 = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "#Task 5: Build a classifier using a linear kernel and plot the decision making function\n",
    "\n",
    "clf = SVC(kernel='linear', gamma = 0.1).fit(X2, y2)\n",
    "\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y2, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.show()\n",
    "\n",
    "# These results should look wrong so we will try something else\n",
    "\n",
    "# Consider projecting our data into a 3D plane\n",
    "r = np.exp(-(X2 ** 2).sum(1))\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(X2[:, 0], X2[:, 1], r, c=y2, s=50, cmap='autumn')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('r')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Looking at the data it is now clear to see that we could draw a linear plane through\n",
    "# it in the 3D space and classify the data. We can then project back to the 2D\n",
    "# space. This is what the 'rbf' kernel does.\n",
    "\n",
    "#Task 6: Try building a classifier using the 'rbf' kernel\n",
    "clf = SVC(kernel='rbf', C=1E6, gamma = 0.1)\n",
    "clf.fit(X2, y2)\n",
    "\n",
    "\n",
    "plt.scatter(X2[:, 0], X2[:, 1], c=y2, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Task 7: Go back to your original dataset (ie. make blobs) and try using different kernels \n",
    "# to build the classifier and plot the results\n",
    "# Compare the differences between the models\n",
    "\n",
    "for ker in ['linear', 'poly', 'rbf']:\n",
    "    model = SVC(kernel = ker, C=1E10, gamma = 0.1)\n",
    "    model.fit(X, y)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model)\n",
    "    plt.show()\n",
    "\n",
    "## So far we have looked at clearly delineated data. Consider the following dataset\n",
    "## where the margins are less clear\n",
    "\n",
    "X3, y3 = make_circles(n_samples=100, factor=0.2, noise = 0.35)\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=y3, s=50, cmap='autumn')\n",
    "plt.show()\n",
    "\n",
    "## SVM has a tuning parameter C which softerns the margins. For very large C, \n",
    "## the margin is hard, and points cannot lie in it. For smaller $C$, the margin \n",
    "# is softer, and can grow to encompass some points.\n",
    "\n",
    "# Task 8: Try experimenting with different values of C and see what different\n",
    "# results you get\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='rbf', C=C, gamma = 0.1).fit(X3, y3)\n",
    "    axi.scatter(X3[:, 0], X3[:, 1], c=y3, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)\n",
    "\n",
    "plt.show()    \n",
    "    \n",
    "# Task 9: Use GridSearchCV (hint: from sklearn.model_selection import GridSearchCV)\n",
    "# to find the optimum parameters for C. \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 5, 10, 100], 'gamma': [0.01, 0.1, 0.3, 0.5]}\n",
    "\n",
    "model = SVC(kernel='rbf', C=1, gamma = 0.1).fit(X3, y3)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "grid.fit(X3, y3)\n",
    "print(grid.best_params_)\n",
    "\n",
    "model = grid.best_estimator_\n",
    "\n",
    "\n",
    "plt.scatter(X3[:, 0], X3[:, 1], c=y3, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics\n",
    "\n",
    "import graphviz \n",
    "\n",
    "# Helper function to plot the decision tree. This uses the graphviz library.\n",
    "def plot_tree(graph, feature_names=None, class_names=None):\n",
    "    '''\n",
    "    This method takes a DecisionTreeClassifier object, along with a list of feature names and target names\n",
    "    and plots a tree. The feature names and class names can be left empty; they are just there for labelling \n",
    "    '''\n",
    "    dot_data = export_graphviz(graph, out_file=None, \n",
    "                      feature_names=feature_names,  \n",
    "                      class_names=class_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True) \n",
    "    \n",
    "    graph = graphviz.Source(dot_data)\n",
    "    \n",
    "    return graph\n",
    "# B1) We initialise the DecisionTreeClassifier object. \n",
    "    # We can set our hyperparameters here if necessary; we'll look into this in a bit.\n",
    "base_model = DecisionTreeClassifier() #criterion='entropy' in the brackets if you want to use the entropy criteria\n",
    "\n",
    "# B2) DecisionTreeClassifier has a fit method, \n",
    "    # which takes the train X and train y to learn a tree. The necessary optimisation is done here. \n",
    "fitted_base_model = base_model.fit(iris_X, iris_y)\n",
    "\n",
    "# B3) Use the helper function defined above to plot the learned model.\n",
    "plot_tree(fitted_base_model, iris.feature_names, iris.target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as per the [documentation](https://scikit-learn.org/stable/modules/tree.html), DecisionTreeClassfier implements a modified version of CART. You can look at documentation for details about the particular algorithm scikit-learn implements. \n",
    "\n",
    "Since it implements CART, by default, DecisionTreeClassifier uses the Gini Index to measure the leaf impurity. You can also use the entropy information gain by setting `criterion='entropy'` when initialising the object. \n",
    "\n",
    "Other options available can be seen [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). A key option is the `max_depth`. This defines the maximum depth of the tree. If this isn't set, as above, the algorithm will continue until all leaves are pure, or until all leaves contain less than `min_samples_split` samples; this is another setting we can set. \n",
    "\n",
    "In other words, `max_depth` reduces the depth of the full tree seen above until the longest path in the tree is equal to `max_depth`. Pruning simplifies the model and makes it more interpretable to humans, and also prevents overfitting. \n",
    "\n",
    "Lets look at how the `max_depth` affects the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test-train data split. \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We have set the random seed to be 2, by setting the random_state parameter. \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3, random_state=2) # 30% is test data\n",
    "\n",
    "# Initialise a new model that uses the default `max_depth`. The code pattern is the same as above.\n",
    "base_model = DecisionTreeClassifier() #The maximum depth of the tree. \n",
    "    # default = None\n",
    "    # None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "fitted_base_model = base_model.fit(iris_X, iris_y)\n",
    "plot_tree(fitted_base_model, iris.feature_names, iris.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill in the blanks below.\n",
    "#Choosing the hyperparameters by guesswork\n",
    "# B4) Create a DecisionTreeclassifier object with the correct hyperparameters. This is your model.\n",
    "    # Tim suggests doing this through cross validation\n",
    "model_2 = DecisionTreeClassifier(max_depth=6, max_features=3)\n",
    "\n",
    "# B5) Use the DecisionTreeclassifier.fit(X, y) function to optimise the model.\n",
    "model_2_fitted = model_2.fit(X_train, y_train)\n",
    "plot_tree(model_2_fitted, iris.feature_names, iris.target_names)\n",
    "\n",
    "# B6) Use the DecisionTreeclassifier.predict(X) to make predictions\n",
    "model_2_y_pred = model_2.predict(X_test)\n",
    "\n",
    "# B7) Use metrics.accuracy_score(y_test, y_predictions) to compute accuracy scores. Print it.\n",
    "full_model_accuracy =  metrics.accuracy_score(y_test, model_2_y_pred)\n",
    "print(f'Accuracy: {full_model_accuracy}')\n",
    "\n",
    "# B8) Plot the tree using plot_tree(model, feature_names, class_names)\n",
    "plot_tree(model_2_fitted, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise another model that sets `max_depth=3`. The code pattern is the same as above.\n",
    "\n",
    "# B4)\n",
    "small_model = DecisionTreeClassifier(max_depth=3)\n",
    "# B5)\n",
    "small_model = model_2.fit(iris_X, iris_y)\n",
    "\n",
    "# B7)\n",
    "small_model_accuracy = metrics.accuracy_score(y_test, model_2_y_pred)\n",
    "print(f'Accuracy: {small_model_accuracy}')\n",
    "# B6)\n",
    "plot_tree(small_model, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that our accuracy has improved, albeit slightly.\n",
    "Note however, that this is dependent on the random seed that we used (since we only ran this once).\n",
    "You can try changing the random_state parameter above when we used the train_test_split function, and see if the result above changes.\n",
    "\n",
    "We can now do a more extensive search by doing a grid search to search over several `max_depth` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using the GridSearchCV from sklearn.model_selection. \n",
    "# You can specify the set of `max_depth`s that you want to try by setting `param_grid={'max_depth':[1, 2, 3, 4, 5, 6]}`.\n",
    "# Documentation can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# The code pattern here is similar to the previous sections. \n",
    "# G1) Initiate a GridSearchCV object with the correct model, param_grid, and cv; `cv=k` does a k-fold cross-validation.\n",
    "grid_search_model = GridSearchCV(DecisionTreeClassifier(random_state=2), {'max_depth':[1, 2, 3, 4, 5, 6]}, cv=15,)\n",
    "\n",
    "# G2) use the GridSearchCV.fit(X, y) method to run the grid search with cv. \n",
    "fitted_grid_search_model = grid_search_model.fit(iris_X, iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean accuracy scores. \n",
    "# The fitted GridSearchCV object has an attribute model.cv_results_ (note the underscore) that gives us a dict object with several results from the cross-valiation. \n",
    "# In particular, one of the key value pairs avaiable is 'mean_test_score', which returns the average score for each parameter value over the cv fold. \n",
    "\n",
    "accuracy_scores = fitted_grid_search_model.cv_results_['mean_test_score']\n",
    "print(f\"Mean accurary scores:{accuracy_scores}\")\n",
    "\n",
    "# Plot the best estimator you found\n",
    "# GridSearchCV.best_estimator_ (again, the underscore) returns the model that performed the best. This behaves the same as the model objects from before, so we can plot it.\n",
    "# G3) Get the best model\n",
    "best_tree_model = fitted_grid_search_model.best_estimator_\n",
    "\n",
    "# G4) Plot the best model\n",
    "plot_tree(best_tree_model, iris.feature_names, iris.target_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
