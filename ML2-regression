#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Mar  2 17:31:07 2020

@author: imyyounge
"""

################################################################################
###                         REGRESSION                                       ###
################################################################################
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

os.chdir('/Users/imyyounge/Documents/4_Masters/4_Machine_learning/Nov_2019_Prescribing_Data/Code') 
final = pd.read_csv('final.csv') # Correction: Need to use the dataset outputted by categorical_and_exclusion
def rename_unname(df):
    for col in df:
        if col.startswith('Unnamed'):
            df.drop(col,axis=1, inplace=True)
rename_unname(final)
print(list(final))
final.drop(['left_parent_date'], axis=1, inplace=True) #Drop the columns with NAs so that hopefully things work later
y = final['number_of_patients'] # Then going to need to make this binary
X = final
X.drop(['number_of_patients'], axis=1, inplace = True)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)
print(list(X_train))
    # Check there will be the same proportion of case-controls in the new dataset as original

#Scaling:
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler() # Only need to scale the train, then apply to the test data
scaler.fit(X_train)
scaler.transform(X_train)  # Think this outputs an array

###################################################################################
                                    #REGRESSION
###################################################################################

# Applying the Linear regressor
from sklearn.modelselection import LinearRegression
regress = LinearRegression()
regress.fit(X_train, y_train)
print(regress.intercept_)
coeff_df = pd.DataFrame(regress.coef_, X_train.columns, columns=['Coefficient'])
coeff_df #Look at the coefficients from the model

y_pred = regress.predict(X_test) # Make predictions using this model
reg_output = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print(reg_output.head(15))

# Evaluating the regression output
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

# Plot the outcome of the actual and predicted y values
reg_output['Actual-log'] = np.log(reg_output['Actual'])
reg_output['Predicted-log'] = np.log(reg_output['Predicted'])

sns.regplot(x="Actual", y="Predicted", data=reg_output)
sns.regplot(x="Actual-log", y="Predicted-log", data=reg_output)


###################################################################################
                            # DECISION TREE REGRESSOR
###################################################################################
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error as MSE
dt = DecisionTreeRegressor(max_depth=2, random_state=1)
mse_dt = MSE(y_test, y_pred)
rmse_dt = mse_dt**(1/2)
print(rmse_dt)

from sklearn.tree import export_graphviz
from sklearn import metrics
import graphviz 

    # Helper function to plot the decision tree. This uses the graphviz library.
def plot_tree(graph, feature_names=None, class_names=None):
    '''
    This method takes a DecisionTreeClassifier object, along with a list of feature names and target names
    and plots a tree. The feature names and class names can be left empty; they are just there for labelling 
    '''
    dot_data = export_graphviz(graph, out_file=None, 
                      feature_names=feature_names,  
                      class_names=class_names,  
                      filled=True, rounded=True,  
                      special_characters=True) 
    
    graph = graphviz.Source(dot_data)
    
    return graph

plot_tree(dt)

    # Then copy the code over for dealing with bias and variance issues in regression decision trees


