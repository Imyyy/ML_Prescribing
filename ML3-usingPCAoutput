#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Mar  3 11:43:00 2020

@author: imyyounge
"""
# Then linear regress this and see what you get - does the model do a better explanation now?
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

os.chdir('/Users/imyyounge/Documents/4_Masters/4_Machine_learning/Nov_2019_Prescribing_Data/Code') # Comment this out for the HPC
final = pd.read_csv('final.csv') # Correction: Need to use the dataset outputted by categorical_and_exclusion
def rename_unname(df):
    for col in df:
        if col.startswith('Unnamed'):
            df.drop(col,axis=1, inplace=True)
rename_unname(final)
print(list(final))
final.drop(['left_parent_date'], axis=1, inplace=True) #Drop the columns with NAs so that hopefully things work later
y = final['number_of_patients'] # Then going to need to make this binary
X = final
X.drop(['number_of_patients'], axis=1, inplace = True)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler() # Only need to scale the train, then apply to the test data
scaler.fit(X_train)
scaler.transform(X_train) 

#########################################################################
                            # FITTING PCA
#########################################################################
from sklearn.decomposition import PCA
pca = PCA(n_components=3, random_state=42) #Initialise the PCA, with 2 components
pca.fit(X_train) # Fit the PCA from X_train 
print("PCA Explained variance ratio")
print(pca.explained_variance_ratio_)
print("Cumulative PCA Explained variance ratio")
print(pca.explained_variance_ratio_.cumsum()) # PCA
X_train_pca = pca.transform(X_train) #Need to do the same idea to training data # Create a pandas dataframe
X_train_pca =  pd.DataFrame(X_train_pca)
X_train_pca.columns = ['PC1','PC2', 'PC3']
X_test_pca = pca.transform(X_test) # Transform the X_test set test
X_test_pca =  pd.DataFrame(X_test_pca)
X_test_pca.columns = ['PC1','PC2', 'PC3']

#########################################################################
                            # PLOTTING THE OUTPUT
#########################################################################
sns.regplot(x=X_train_pca["PC1"], y=X_train_pca["PC2"], fit_reg=False)

sns.jointplot(x=X_train_pca["PC1"], y=X_train_pca["PC2"], kind='kde', xlim = [-50000, 0]) # Get kernel density estimation plot

#########################################################################
                            # REGRESSION
#########################################################################
from sklearn.linear_model import LinearRegression
regress = LinearRegression()
regress.fit(X_train_pca, y_train)
print("Regression intercept")
print(regress.intercept_)
pca_regression_coefficients = pd.DataFrame(regress.coef_, columns=['Coefficient']) # get an issue here, not sure why 
pca_regression_coefficients #Look at the coefficients from the model

y_pred_pca = regress.predict(X_test_pca) # Make predictions using this model
reg_output = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_pca})
print(reg_output.head(15))

#########################################################################
                        # EVALUATING REGRESSION
#########################################################################
from sklearn import metrics
print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred_pca))
print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred_pca))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_pca)))

# Plot the outcome of the actual and predicted y values
reg_output['Actual-log'] = np.log(reg_output['Actual'])
reg_output['Predicted-log'] = np.log(reg_output['Predicted'])

sns.regplot(x="Actual", y="Predicted", data=reg_output)
sns.regplot(x="Actual-log", y="Predicted-log", data=reg_output)



